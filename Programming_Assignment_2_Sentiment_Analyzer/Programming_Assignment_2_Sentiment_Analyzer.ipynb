{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Programming Assignment 2 Sentiment Analyzer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/potasali/Machine-Learning/blob/master/Programming_Assignment_2_Sentiment_Analyzer/Programming_Assignment_2_Sentiment_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Jwvo67fwlD",
        "colab_type": "text"
      },
      "source": [
        "# Programming Assignment 2: Sentiment Analyzer\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The purpose of this assignment is to get you familiar with sentiment classification. By the\n",
        "end of this assignment you will have your very own “Sentiment Analyzer”. You are given\n",
        "with Large Movie Review Dataset that contains separate labelled train and test set. Your\n",
        "task is to train a Logistic Regression classifier on train set and report evaluation metrics on\n",
        "test set. \n",
        "\n",
        "Let's start with the necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XhwC3KvfwlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import math \n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwJI3jCqfwlI",
        "colab_type": "text"
      },
      "source": [
        "## 1. Dataset\n",
        "\n",
        "- $x_1$ = count(positive words) ∈ review\n",
        "- $x_2$ = count(negative words) ∈ review\n",
        "- $x_3$ = Star Rating (1-10 scale)\n",
        "- $x_4$ = log(word count of review)\n",
        "- $x_5$ = 1 if “no” ∈ review, 0 otherwise\n",
        "- $x_6$ = 1 if “!” ∈ review, 0 otherwise\n",
        "- $y$ = 1 if positive, 0 otherwise\n",
        "\n",
        "### 1.1 Fetching Reviews\n",
        "\n",
        "The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k pos and 25k neg). There are two top-level directories `[train/, test/]` corresponding to the training and test sets. Each contains `[pos/,neg/]` directories for the reviews with binary labels positive and negative. Within these directories, reviews are stored in text files named following the convention `[[id]_[rating].txt]` where `[id]` is a unique id and `[rating]` is the star rating for that review on a 1-10 scale. For example, the file `[test/pos/200_8.txt]` is the text for a positive-labeled test set example with unique id 200 and star rating 8/10 from IMDb.\n",
        "\n",
        "The reviews and the ratings are loaded from the data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlqEB8bOfwlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function to fetch reviews from the files\n",
        "def getDataset(_path):\n",
        "    \n",
        "    # Creating a dictionary to store each review according to their id\n",
        "    dataset = {}\n",
        "    ratings = []\n",
        "    temp = \"\"\n",
        "\n",
        "    # Using glob module to retrieve files/pathnames.\n",
        "    files = (glob.glob(_path))\n",
        "\n",
        "    # Iterating through each file in the given folder\n",
        "    for file in files:\n",
        "\n",
        "        # Opening the file as read only\n",
        "        with open(file, 'r', encoding=\"utf8\") as myFile:\n",
        "\n",
        "            # Reading review from a file\n",
        "            review = myFile.read()\n",
        "\n",
        "            # Preprocessing the file name before using it as the index for our dictionary\n",
        "            name = file.split('/')[-1]\n",
        "\n",
        "            # Storing review for each file in the dictionary\n",
        "            dataset[name] = review\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Loading the datasets, separating train and test dataset and further separating into two sub-categories, positive and negative reviews.\n",
        "ptrReview = getDataset('Dataset/train/pos/*')\n",
        "ntrReview = getDataset('Dataset/train/neg/*')\n",
        "pteReview = getDataset('Dataset/test/pos/*')\n",
        "nteReview = getDataset('Dataset/test/neg/*')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmnYZ8lqfwlL",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Dataset Preprocessing\n",
        "<ul>\n",
        "    <li>Loading Positive and Negative Lexicons from the dataset file </li>\n",
        "    <li>Finding lexicon count</li>\n",
        "    <li>Finding word count</li>\n",
        "    <li>Finding `no` count</li>\n",
        "    <li>Finding `!` count</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLUjEucdfwlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading Positive Lexicons\n",
        "with open('Dataset/positive-words.txt','r') as myFile:\n",
        "    pLex = myFile.read().split('\\n')\n",
        "\n",
        "# Loading Negative Lexicons\n",
        "with open('Dataset/negative-words.txt','r') as myFile:\n",
        "    nLex = myFile.read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPqDHkL-fwlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data preprocessing function\n",
        "def preprocess(review, pLex, nLex):\n",
        "    X = []\n",
        "\n",
        "    # Iterating through each review\n",
        "    for filename, rev in review.items():\n",
        "\n",
        "        # Initializing the positive and negative lexicon count\n",
        "        x1 = np.float64(0)\n",
        "        x2 = np.float64(0)\n",
        "\n",
        "        # Initializing the word count\n",
        "        count = np.float64(0)\n",
        "\n",
        "        # Converting the review text into lower case\n",
        "        rev = rev.lower()\n",
        "\n",
        "        # Creating a word vector array for each review \n",
        "        revsplit = rev.split()\n",
        "        \n",
        "        for word in revsplit:\n",
        "            \n",
        "            # Counting the number of Positive Lexicons in the review\n",
        "            if word in pLex:\n",
        "                x1 += 1\n",
        "                \n",
        "            # Counting the number of Negative Lexicons in the review\n",
        "            if word in nLex:\n",
        "                x2 += 1\n",
        "            \n",
        "            # Counting the number of words in the review\n",
        "            count += 1\n",
        "        \n",
        "        # Taking log of the word count\n",
        "        x4 = np.log(count)\n",
        "        \n",
        "        # Extracting the rating from the dataset dictionary index\n",
        "        x3 = np.float64(((re.findall(\"_\\d+\", filename)).pop(0))[1:])\n",
        "        \n",
        "        # Checking the occurence of `no` in the review\n",
        "        if 'no' in rev:\n",
        "            x5 = np.float64(1)\n",
        "        else:\n",
        "            x5 = np.float64(0)\n",
        "\n",
        "        # Checking the occurence of `!` in the review\n",
        "        if '!' in rev:\n",
        "            x6 = np.float64(1)\n",
        "        else:\n",
        "            x6 = np.float64(0)\n",
        "\n",
        "        # Storing the features in a feature vector\n",
        "        x = np.array([x1, x2, x3, x4, x5, x6])\n",
        "        X.append(x)\n",
        "   \n",
        "    X = np.array(X)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7eKOC35fwlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Passing the datasets to the preprocessing function \n",
        "ptrX = preprocess(ptrReview, pLex, nLex)\n",
        "ntrX = preprocess(ntrReview, pLex, nLex)\n",
        "pteX = preprocess(pteReview, pLex, nLex)\n",
        "nteX = preprocess(nteReview, pLex, nLex)\n",
        "\n",
        "# Extracting labels from our datasets\n",
        "ptrY = np.ones(ptrX.shape[0]).reshape(-1,1)\n",
        "ntrY = np.zeros(ntrX.shape[0]).reshape(-1,1)\n",
        "pteY = np.ones(pteX.shape[0]).reshape(-1,1)\n",
        "nteY = np.zeros(nteX.shape[0]).reshape(-1,1)\n",
        "\n",
        "# Combining the negative and positive dataset to get train and test data further divided by X and Y\n",
        "Xtr = np.concatenate((ptrX, ntrX))\n",
        "Ytr = np.concatenate((ptrY, ntrY))\n",
        "Xte = np.concatenate((pteX, nteX))\n",
        "Yte = np.concatenate((pteY, nteY))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQqGky2fwlU",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Shuffling Dataset\n",
        "Shuffling data serves the purpose of reducing variance and making sure that models remain general and overfit less.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW3-SgY0fwlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle numbers from 0 to the size of the dataset\n",
        "def createShuffle(size):\n",
        "    array = np.array([i for i in range(size)])\n",
        "    random.shuffle(array)\n",
        "    return array\n",
        "    \n",
        "# Sorting the datasets index according to the previously shuffled array\n",
        "def shuffleArray(posArray, arrayToShuffle):\n",
        "    size = len(posArray)\n",
        "\n",
        "    # Initializing a shuffled array\n",
        "    shuffledArray = np.empty_like(arrayToShuffle) \n",
        "\n",
        "    # Iterating through the entire dataset and sorting the dataset using the previously shuffled array as the index\n",
        "    for i in range(size):\n",
        "        shuffledArray[i] = arrayToShuffle[posArray[i]]\n",
        "    return shuffledArray\n",
        "\n",
        "m = len(ptrReview) + len(ntrReview)\n",
        "array = createShuffle(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d_q35jlfwlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtr = shuffleArray(array, Xtr)\n",
        "Ytr = shuffleArray(array, Ytr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CaZlZOjfwla",
        "colab_type": "text"
      },
      "source": [
        "## 2. Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb2nYZxtfwla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sigmoid function takes an input z of any real number and returns an output value in the range of 0 and 1\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Defining epsilon\n",
        "epsilon = np.float( 1e-100 )\n",
        "\n",
        "# Cross Entropy loss measures the performance of our classification model.\n",
        "def crossEntropyLoss(h_x, Y):\n",
        "    total_cost = (-Y * np.log(h_x + epsilon) - (1 - Y) * np.log(1 - h_x + epsilon)).mean()\n",
        "    return total_cost\n",
        "\n",
        "# Hypothesis Function h(x) predicts the label of a set of data\n",
        "def hypothesis(X, Theta):\n",
        "\n",
        "    # Taking dot product of the Feature Vector 'X' and the parameters 'Thetas'\n",
        "    hyp = sigmoid(np.dot(X, Theta).reshape(-1,1))\n",
        "\n",
        "    return hyp\n",
        "\n",
        "#Predicting Y\n",
        "def predict(X, Theta):\n",
        "\n",
        "    # Using our model to predict the label of the set of data\n",
        "    h_x = hypothesis(X, Theta)\n",
        "    predictions = []\n",
        "\n",
        "    # Classifying the predicted value into {0,1} by separating at the 0.5 mark\n",
        "    for h_xi in h_x:\n",
        "        if h_xi[0] <= 0.5:\n",
        "            predictions.append(0)\n",
        "        else:\n",
        "            predictions.append(1)\n",
        "    \n",
        "    # Storing the predictions in an array\n",
        "    predictions = np.array(predictions)\n",
        "    predictions = predictions.reshape(-1,1)\n",
        "    return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5eiUzN1fwld",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsmVCsSmfwld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch gradient function\n",
        "def batchGradientDescent(X, Y, n_epochs, alpha):\n",
        "\n",
        "    # Inittializing the parameters with ones\n",
        "    Theta = np.ones(X.shape[1]).reshape(-1, 1)\n",
        "\n",
        "    # Initializing the cost\n",
        "    J = np.zeros(n_epochs)\n",
        "\n",
        "    # Iterating for a predefined epoch\n",
        "    for i in range(n_epochs):\n",
        "\n",
        "        # Predicting the labels using our model\n",
        "        h_x = hypothesis(X, Theta)\n",
        "\n",
        "        # Calculating the cross entropy loss for our model for the entire data set\n",
        "        J[i] = crossEntropyLoss(h_x, Y)\n",
        "\n",
        "        # Updating Theta (parameters) using gradient descent with a predefined alpha\n",
        "        Theta -= alpha * np.dot(X.T, (h_x - Y)) / X.shape[0]\n",
        "\n",
        "    return J, Theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP2__BMzfwlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 500\n",
        "alpha = 0.01\n",
        "J_batch, T_batch = batchGradientDescent(Xtr, Ytr, n_epochs, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPSfRXaUfwli",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KyiWWGxfwli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def stochastic_gradient_descent(X, Y, n_epochs, alpha):\n",
        "    \n",
        "    # Inittializing the parameters with ones\n",
        "    Theta = np.ones(X.shape[1]).reshape(-1, 1)\n",
        "    \n",
        "    # Initializing the cost\n",
        "    J = np.zeros(n_epochs * m)\n",
        "\n",
        "    index = 0\n",
        "\n",
        "    # Iterating for a predefined epoch\n",
        "    for i in range(n_epochs):\n",
        "\n",
        "        # Iterating through each data entry\n",
        "        for x, y in zip(X, Y):\n",
        "\n",
        "            # Predicting the label using our model\n",
        "            h_x = hypothesis(x.reshape(1, -1), Theta)\n",
        "            \n",
        "            # Calculating the cross entropy loss for our model\n",
        "            J[index] = crossEntropyLoss(h_x, y.reshape(1, -1))\n",
        "\n",
        "            index +=1      \n",
        "            \n",
        "            # Updating Theta (parameters) using gradient descent with a predefined alpha for each data entry\n",
        "            Theta -= alpha * np.dot(x.reshape(1,-1).T, (h_x - y.reshape(1,-1))) / x.reshape(1,-1).shape[0] \n",
        "\n",
        "    return J, Theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lqyOe6afwlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "J_stochastic, T_stochastic = stochastic_gradient_descent(Xtr, Ytr, n_epochs, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jl0s-ZTfwlm",
        "colab_type": "text"
      },
      "source": [
        "## 3. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B58sew2nfwln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_batch = predict(Xte, T_batch)\n",
        "predict_stochastic = predict(Xte, T_batch)\n",
        "\n",
        "def eval(prediction, expected):\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "\n",
        "    for E, P in zip(expected, prediction):\n",
        "        if E == 0:\n",
        "\n",
        "            # Prediction = 0 and Expected = 0\n",
        "            if P == E:\n",
        "                tn += 1\n",
        "\n",
        "            # Prediction = 1 and Expected = 0\n",
        "            else:\n",
        "                fp += 1\n",
        "\n",
        "        if E == 1:\n",
        "\n",
        "            # Prediction = 0 and Expected = 1\n",
        "            if P == E:\n",
        "                tp += 1\n",
        "\n",
        "            # Prediction = 1 and Expected = 1\n",
        "            else:\n",
        "                fn += 1\n",
        "                \n",
        "    #Precision\n",
        "    precision = float(tp) / float(tp + fp)\n",
        "    print(\"Precision: \", precision)\n",
        "    \n",
        "    #Recall\n",
        "    recall = float(tp) / float(tp + fn)\n",
        "    print(\"Recall: \", recall)\n",
        "    \n",
        "    #Accuracy\n",
        "    accuracy = float(tp + tn) / float(tp + fn + tn + fp)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    \n",
        "    #F1 score\n",
        "    f1_score = (2 * precision * recall) / (precision + recall)\n",
        "    print(\"F1 Score: \", f1_score)\n",
        "    \n",
        "    #Confusion Matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(tp, fp)\n",
        "    print(fn, tn)\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is_2fawAfwlp",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Batch Gradient Descent Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4F2LtCVfwlp",
        "colab_type": "code",
        "colab": {},
        "outputId": "aabcd8a1-f5fd-49f3-c52c-7d9cee6a1abe"
      },
      "source": [
        "eval(predict_batch, Yte)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  0.9879996821107844\n",
            "Recall:  0.99456\n",
            "Accuracy:  0.99124\n",
            "F1 Score:  0.9912689869632818\n",
            "\n",
            "Confusion Matrix:\n",
            "12432 151\n",
            "68 12349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkLfOblNfwlt",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Stochastic Gradient Descent Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZEqQaKdfwlu",
        "colab_type": "code",
        "colab": {},
        "outputId": "37ad57d4-b50d-400d-ea34-02ccd464d749"
      },
      "source": [
        "eval(predict_stochastic, Yte)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  0.9879996821107844\n",
            "Recall:  0.99456\n",
            "Accuracy:  0.99124\n",
            "F1 Score:  0.9912689869632818\n",
            "\n",
            "Confusion Matrix:\n",
            "12432 151\n",
            "68 12349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpxgCfBtfwlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
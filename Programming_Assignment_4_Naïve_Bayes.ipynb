{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Programming Assignment 4 Naïve Bayes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/potasali/Machine-Learning/blob/master/Programming_Assignment_4_Na%C3%AFve_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdPvhkvFGROP",
        "colab_type": "text"
      },
      "source": [
        "# Programming Assignment 4: Naïve Bayes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw6vSDnZGROQ",
        "colab_type": "text"
      },
      "source": [
        "## Problem\n",
        "The purpose of this assignment is to get you familiar with multinomial sentiment\n",
        "classification. By the end of this assignment you will have your very own “Sentiment\n",
        "Analyzer”. You are given with Twitter US Airline Sentiment Dataset that contains around\n",
        "14,640 tweets about airlines labelled as positive, negative and neutral. Your task is to train\n",
        "a Naïve Bayes classifier on this dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZhOrkkaGROS",
        "colab_type": "text"
      },
      "source": [
        "## 1. Dataset Splitting\n",
        "Instead of a usual random split, we will split the dataset in a stratified fashion. Stratified\n",
        "splitting ensure that the train and test sets have approximately the same percentage of\n",
        "samples of each target class as the complete set. For example, in an 80-20 stratified split\n",
        "80% samples of each class will be in train set and 20% in test set.\n",
        "Implement stratified split and do the 80-20 train-test split of the provided dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKHcs-1-GROT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import math \n",
        "import pandas as pd\n",
        "import string\n",
        "import heapq\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWQEEzN_GROW",
        "colab_type": "code",
        "colab": {},
        "outputId": "8d126ec5-94f7-4668-f9dd-78a66000d0b2"
      },
      "source": [
        "# Loading the dataset\n",
        "df = pd.read_csv (r'Tweets.csv')\n",
        "\n",
        "# Shuffling the dataset\n",
        "df.sample(frac=1)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@USAirways Is there a phone line to call into ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>@united Bag was finally delivered and intact. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "      <td>@usairways Thanks to Kevin and team at F38ish ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>negative</td>\n",
              "      <td>@AmericanAir Yes, talked to them. FLL says is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14635</td>\n",
              "      <td>positive</td>\n",
              "      <td>@southwestair Great job celebrating #MardiGras...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14636</td>\n",
              "      <td>negative</td>\n",
              "      <td>@JetBlue I have been on phone with rep for ove...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14637</td>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica @SSal thanks!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14638</td>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica Thank you!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14639</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@USAirways If I’m unable to use my departing f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14640 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      airline_sentiment                                               text\n",
              "0               neutral  @USAirways Is there a phone line to call into ...\n",
              "1              positive  @united Bag was finally delivered and intact. ...\n",
              "2              positive  @usairways Thanks to Kevin and team at F38ish ...\n",
              "3              negative  @AmericanAir Yes, talked to them. FLL says is ...\n",
              "4              negative  @VirginAmerica and it's a really big bad thing...\n",
              "...                 ...                                                ...\n",
              "14635          positive  @southwestair Great job celebrating #MardiGras...\n",
              "14636          negative  @JetBlue I have been on phone with rep for ove...\n",
              "14637          positive                       @VirginAmerica @SSal thanks!\n",
              "14638          positive                         @VirginAmerica Thank you!!\n",
              "14639           neutral  @USAirways If I’m unable to use my departing f...\n",
              "\n",
              "[14640 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBKz5qb5GROa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dividing the dataset into train and test data and then furthe dividing it into X and Y. \n",
        "def dataSplit(df):\n",
        "    \n",
        "    # Exctracting data for each class\n",
        "    neu = df[df['airline_sentiment'] == 'neutral']\n",
        "    pos = df[df['airline_sentiment'] == 'positive']\n",
        "    neg = df[df['airline_sentiment'] == 'negative']\n",
        "\n",
        "    # Splitting the dataset into train and test data using 80:20 ratio\n",
        "\n",
        "    # Using the first 80% data entries as train data\n",
        "    neu_train = neu[0:int( len(neu)*0.8 )]\n",
        "    pos_train = pos[0:int( len(pos)*0.8 )]\n",
        "    neg_train = neg[0:int( len(neg)*0.8 )]\n",
        "    \n",
        "    # Using the last 20% data entries as test data\n",
        "    neu_test = neu[int(len(neu)*0.8): len(neu)]\n",
        "    pos_test = pos[int(len(pos)*0.8): len(pos)]\n",
        "    neg_test = neg[int(len(neg)*0.8): len(neg)]\n",
        "\n",
        "    # Combining the three classes to get train and test data further divided by X and Y\n",
        "    X_train = np.concatenate((neu_train['text'], pos_train['text'], neg_train['text']))\n",
        "    Y_train = np.concatenate((neu_train['airline_sentiment'], pos_train['airline_sentiment'], neg_train['airline_sentiment']))\n",
        "    X_test = np.concatenate((neu_test['text'], pos_test['text'], neg_test['text']))\n",
        "    Y_test = np.concatenate((neu_test['airline_sentiment'], pos_test['airline_sentiment'], neg_test['airline_sentiment']))\n",
        "    \n",
        "    return X_train, Y_train, X_test, Y_test, np.array([neu_train['text'], pos_train['text'], neg_train['text']]), np.array( [neu_test['text'], pos_test['text'], neg_test['text']])\n",
        "\n",
        "X_train, Y_train, X_test, Y_test, x_train, x_test  = dataSplit(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssbChCRpGROc",
        "colab_type": "text"
      },
      "source": [
        "## 2. Dataset Preprocessing\n",
        "We’ll represent a tweet as a bag-of-words, that is, an unordered set of words with their\n",
        "position ignored, keeping only their frequency in the tweet. <br>\n",
        "Please note that in our case the vocabulary might be in thousands, so we will use text cleaning techniques such as ignore case, punctuation and frequent (stop) words like “a”, ”an”, “the” etc. to reduce the size of vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nTpV3pHGROd",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 One Hot Encoding & Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMVmrqpDGROd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One hot encoding for each label in the training dataset\n",
        "# Using \n",
        "#   first column as neutral\n",
        "#   second column as positive\n",
        "#   third column as negative\n",
        "def oheEncoding(data, arr):\n",
        "    '''\n",
        "    Returns a matrix where each sample in y is represented\n",
        "           as a row, and each column represents the class label in\n",
        "           the one-hot encoding scheme.\n",
        "    '''\n",
        "    for label in data:\n",
        "\n",
        "        if label == 'neutral':\n",
        "            arr.append([1,0,0])\n",
        "\n",
        "        elif label == 'positive':\n",
        "            arr.append([0,1,0])\n",
        "\n",
        "        elif label == 'negative':\n",
        "            arr.append([0,0,1])\n",
        "\n",
        "    arr = np.asarray(arr)\n",
        "\n",
        "# label encoding for each label in the test dataset\n",
        "def labelEncoding(data, arr):\n",
        "\n",
        "    for label in data:\n",
        "\n",
        "        # Labeling neutral as 0\n",
        "        if label == 'neutral':\n",
        "            arr.append(0)\n",
        "\n",
        "        # Labeling positive as 1\n",
        "        elif label == 'positive':\n",
        "            arr.append(1)\n",
        "        \n",
        "        # Labeling negative as 2\n",
        "        elif label == 'negative':\n",
        "            arr.append(2)\n",
        "            \n",
        "    arr = np.asarray(arr)\n",
        "\n",
        "def yProcess(Y_train, Y_test):\n",
        "    ytr = []\n",
        "    yte = []\n",
        "    oheEncoding(Y_train, ytr)\n",
        "    labelEncoding(Y_test, yte)\n",
        "    return ytr, yte\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XrhbQQTGROg",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Data Cleaning & Bag of Words Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhCJZTs4GROh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Cleaning the dataset\n",
        "def dataCleaning(data):\n",
        "    arr = []\n",
        "\n",
        "    # Iterating through the entire X data\n",
        "    for text in data:\n",
        "\n",
        "        # Converting text into lower case\n",
        "        text = text.lower()\n",
        "\n",
        "        # Replaces all non word characters (this excludes characters from a to Z and digits from 0-9) with a space\n",
        "        text = re.sub(r'\\W',' ',text)\n",
        "\n",
        "        # Replaces multiple spaces with a single space\n",
        "        text = re.sub(r'\\s+',' ',text)\n",
        "\n",
        "        # Appends the preprocessesed data to an array\n",
        "        arr.append(text)\n",
        "\n",
        "    arr = pd.DataFrame(arr, columns=[\"text\"])\n",
        "    return arr\n",
        "\n",
        "# Finding the frequently used words from the dataset\n",
        "\n",
        "def dictionary(tweets):\n",
        "\n",
        "    # Initializing the word dictionary\n",
        "    wordfreq = {}\n",
        "\n",
        "    # Iterating through the entire X data\n",
        "    for tweet in tweets:\n",
        "\n",
        "        # Tokenizing each word from the data\n",
        "        tokens = tweet.split()\n",
        "\n",
        "        # Iterating through every word\n",
        "        for token in tokens:\n",
        "\n",
        "            # Update the count of the word in the dictionary while dynamically adding new words to the dictionary\n",
        "            if token not in wordfreq.keys():\n",
        "                wordfreq[token] = 1\n",
        "            else:\n",
        "                wordfreq[token] += 1\n",
        "\n",
        "    return wordfreq\n",
        "\n",
        "# Creating a bag of words vector\n",
        "def bow(tweets, vocab):\n",
        "\n",
        "    # Initializing the bag of word vector\n",
        "    tweet_vectors = []\n",
        "\n",
        "    # Iterating through the entire X data\n",
        "    for tweet in tweets:\n",
        "\n",
        "        # Tokenizing each word from the data\n",
        "        tweet_tokens = tweet.split()\n",
        "        sent_vec = []\n",
        "\n",
        "        # Iterating through every word from the entire vocabulary\n",
        "        for token in vocab:\n",
        "\n",
        "            # Update the count of the each word in the tweet for the given vocabulary\n",
        "            if token in tweet_tokens:\n",
        "                sent_vec.append(tweet.count(token))\n",
        "            else:\n",
        "                sent_vec.append(0)\n",
        "\n",
        "        # Appending the counts to the bag of word vector   \n",
        "        tweet_vectors.append(sent_vec)\n",
        "    return tweet_vectors\n",
        "\n",
        "def xProcess(X_train, X_test, x_train, x_test):\n",
        "    \n",
        "    # Passing the datasets through the dataCleaing function \n",
        "    xtr = dataCleaning(X_train)\n",
        "    xte = dataCleaning(X_test)\n",
        "\n",
        "    # Initializing an array to store preprocessed data separated by classes\n",
        "    _xtr = []\n",
        "\n",
        "    # Passing the classes separately through the dataCleaing function \n",
        "    for i in range(3):\n",
        "        _xtr.append(dataCleaning(x_train[i]))\n",
        "\n",
        "    # Finding the vocabulary for the train data\n",
        "    wordfreq = dictionary(xtr.text)\n",
        "\n",
        "    # Sorting the vocabulary using heapsort using word count\n",
        "    mostfreq = heapq.nlargest(5989, wordfreq, key=wordfreq.get)\n",
        "\n",
        "    # Removing stopwords fromo the vocabulary\n",
        "    mostfreq_stop = [word for word in mostfreq if not word in stopwords.words()]\n",
        "\n",
        "     # Finding the bag of words for the classes separately\n",
        "    for i in range(3):\n",
        "        _xtr[i] = bow(_xtr[i].text, mostfreq_stop)\n",
        "        _xtr[i] = pd.DataFrame(_xtr[i])\n",
        "        _xtr[i] = _xtr[i].to_numpy()\n",
        "\n",
        "    xte = bow(xte.text, mostfreq_stop)\n",
        "    xte = pd.DataFrame(xte)    \n",
        "    xte = xte.to_numpy()\n",
        "    \n",
        "    return _xtr, xte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5X8FzL_GROk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns separate preprocessed dataset for each class\n",
        "def dataSetPreProcess(X_train, X_test, Y_train, Y_test, x_train, x_test):   \n",
        "    ytr, yte = yProcess(Y_train, Y_test)\n",
        "    xtr, xte = xProcess(X_train, X_test, x_train, x_test)\n",
        "    return xtr, ytr, xte, yte\n",
        "xtr, ytr, xte, yte = dataSetPreProcess(X_train, X_test, Y_train, Y_test, x_train, x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd7W1yHEGROo",
        "colab_type": "code",
        "colab": {},
        "outputId": "723e21bd-f178-4018-bf51-9f1374b1dc90"
      },
      "source": [
        "print(xtr[0].shape)\n",
        "print(xtr[1].shape)\n",
        "print(xtr[2].shape)\n",
        "print(xte.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2479, 5729)\n",
            "(1890, 5729)\n",
            "(7342, 5729)\n",
            "(2929, 5729)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94RaSb0EGROr",
        "colab_type": "text"
      },
      "source": [
        " ## 3. Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOpoYf9CGROr",
        "colab_type": "text"
      },
      "source": [
        "### 3.1  Training Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZmyuehbGROs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainNaiveBayes(X_train, xtr):\n",
        "    \n",
        "    # number of samples in X\n",
        "    N_doc = (X_train.shape)[0]\n",
        "\n",
        "    # number of samples from X in each class\n",
        "    N_c = np.array([(xtr[0].shape)[0], (xtr[1].shape)[0], (xtr[2].shape)[0]])\n",
        "\n",
        "    # Finding the probablity of each prior\n",
        "    prob_prior = np.array([N_c[0], N_c[1], N_c[2]])\n",
        "    prob_prior = prob_prior / N_doc\n",
        "\n",
        "    # Finding the log of the priors\n",
        "    log_prior = np.log(prob_prior)\n",
        "    \n",
        "\n",
        "    class_sum = []\n",
        "    class_total = []\n",
        "    likelihood = []\n",
        "\n",
        "    # Iterating over each class\n",
        "    for i in range(3):\n",
        "        \n",
        "        # Finding the total count of each word\n",
        "        class_sum.append(np.sum(xtr[i], axis = 0)) \n",
        "        \n",
        "         # Applying Add-1 smoothing \n",
        "        class_sum[i] = class_sum[i] + 1        \n",
        "        class_total.append(np.sum(class_sum))\n",
        "        \n",
        "        # Finding the probability of each word\n",
        "        likelihood.append(class_sum[i] / class_total[i])\n",
        "    \n",
        "    # Finding the log of the probabilty of each word\n",
        "    log_likelihood = []\n",
        "    for i in range(3):\n",
        "        log_likelihood.append(np.log(likelihood[i]))\n",
        "    \n",
        "    # returns log P(c) and log P(w|c)\n",
        "    return log_prior, log_likelihood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6csz3VXGROu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_prior, log_likelihood = trainNaiveBayes(X_train, xtr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh9u-57nGROw",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Testing Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HLlZ-amGROx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testNaiveBayes(log_prior, log_likelihood, xte):\n",
        "    predictions = []\n",
        "\n",
        "    # Finding the probability of the test sample being in one of the classes\n",
        "    for i in range(3):\n",
        "        class_prob = np.dot(xte,log_likelihood[i])\n",
        "        class_prob = class_prob + log_prior[i]\n",
        "        predictions.append(class_prob)\n",
        "    \n",
        "    prediction_label = []\n",
        "    m = (xte.shape)[0]\n",
        "\n",
        "    # Iterating through the entire test data\n",
        "    for i in range(m):\n",
        "\n",
        "        # Choosing the class with the maximum probability\n",
        "        _max = max(predictions[0][i],predictions[1][i],predictions[2][i])\n",
        "        for j in range(3):\n",
        "            if _max == predictions[j][i]:\n",
        "                prediction_label.append(j)\n",
        "                \n",
        "    print(len(prediction_label))\n",
        "    \n",
        "    return prediction_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD9d9ptdGRO1",
        "colab_type": "code",
        "colab": {},
        "outputId": "23bb2706-241d-460c-984e-7893cbe0ac2e"
      },
      "source": [
        "prediction_label = testNaiveBayes(log_prior, log_likelihood, xte)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMA2bHZYGRO3",
        "colab_type": "text"
      },
      "source": [
        "## 4. Evaluation report\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oeLTPWYGRO4",
        "colab_type": "code",
        "colab": {},
        "outputId": "2ce4b3c6-3cf3-4898-b387-00877b840c41"
      },
      "source": [
        "def eval(prediction, expected):\n",
        "    \n",
        "    # Initializing the confusion matrix along with all the outcomes\n",
        "    tp = [0,0,0]\n",
        "    tn = [0,0,0]\n",
        "    fp = [0,0,0]\n",
        "    fn = [0,0,0]\n",
        "    CM = np.full((3,3), 0)\n",
        "\n",
        "    # Updating the confusion matrix\n",
        "    for E, P in zip(expected, prediction): \n",
        "        CM[P][E] += 1\n",
        "    \n",
        "    # Updating values for the true-positives, true-negatives, false-positives, false-negatives\n",
        "    for i in range(3):\n",
        "        tp[i] = CM[i][i]\n",
        "        fp[i] = (CM[:, i]).sum() - tp[i]\n",
        "        fn[i] = (CM[i, :]).sum() - tp[i]\n",
        "        tn[i] = (CM.sum()) - tp[i] - fp[i] - fn[i]\n",
        "\n",
        "    # Displaying the Confusion Matrix\n",
        "    CM = pd.DataFrame(data=CM)\n",
        "    CM.columns = ['neutral', 'positive', 'negative']\n",
        "    CM.rename(index={0:'neutral',1:'positive',2:'negative'}, inplace=True)\n",
        "    print(\"Confusion Matrix\\n\\n\", CM)\n",
        "    \n",
        "    # Finding the pooled confusion matrix\n",
        "    poolCM = np.array([[sum(tp), sum(fp)],[sum(fn), sum(tn)]])\n",
        "    print(\"\\nPooled: \\n\", poolCM)\n",
        "    \n",
        "    # Micro-Averaging Precision \n",
        "    print(\"\\nMicro-Average Precision: \", (sum(tp)/(sum(fp)+sum(tp))))\n",
        "    \n",
        "    # Micro-Averaging Recall\n",
        "    print(\"Micro-Average Recall: \", (sum(tp)/(sum(fn)+sum(tp))))\n",
        "    \n",
        "    # Micro-Averaging Accuracy\n",
        "    print(\"Micro-Average Accuracy: \", ((sum(tp)+sum(tn))/(sum(fn)+sum(tp)+sum(fp)+sum(tn))))\n",
        "    \n",
        "    # Micro-Averaging F1 score\n",
        "    miP = (sum(tp)/(sum(fp)+sum(tp)))\n",
        "    miR = (sum(tp)/(sum(fn)+sum(tp)))\n",
        "    f1_score = (2 * miP * miR) / (miP + miR)\n",
        "    print(\"Micro-Average F1-Score: \", f1_score)\n",
        "    \n",
        "    # Finding Precison, Accuracy and Recall for each class\n",
        "    maP = []\n",
        "    maR = []\n",
        "    maA = []\n",
        "    for i in range(3):\n",
        "        maP.append(tp[i] / (tp[i] + fp[i]))\n",
        "        maR.append(tp[i] / (tp[i] + fn[i]))\n",
        "        maA.append((tp[i] + tn[i]) / (tp[i] + tn[i] + fp[i] + fn[i]))\n",
        "\n",
        "    # Macro-Average Precision\n",
        "    _maP = (sum(maP))/3\n",
        "    print(\"\\nMacro-Average Precision: \", _maP)\n",
        "\n",
        "    # Macro-Average Recall\n",
        "    _maR = (sum(maR))/3\n",
        "    print(\"Macro-Average Recall: \", _maR)\n",
        "\n",
        "    # Macro-Average Accuracy\n",
        "    _maA = (sum(maA))/3\n",
        "    print(\"Macro-Average Accuracy: \", _maA)\n",
        "\n",
        "    # Macro-Average F1 score\n",
        "    f1_score = (2 * _maP * _maR) / (_maP + _maR)\n",
        "    print(\"Macro-Average F1-Score: \", f1_score)\n",
        "\n",
        "eval(prediction_label, yte)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "\n",
            "           neutral  positive  negative\n",
            "neutral       510       307       681\n",
            "positive        7        68         1\n",
            "negative      103        98      1154\n",
            "\n",
            "Pooled: \n",
            " [[1732 1197]\n",
            " [1197 4661]]\n",
            "\n",
            "Micro-Average Precision:  0.5913280983270741\n",
            "Micro-Average Recall:  0.5913280983270741\n",
            "Micro-Average Accuracy:  0.7275520655513827\n",
            "Micro-Average F1-Score:  0.5913280983270741\n",
            "\n",
            "Macro-Average Precision:  0.531628054567613\n",
            "Macro-Average Recall:  0.6956170990984031\n",
            "Macro-Average Accuracy:  0.6956170990984031\n",
            "Macro-Average F1-Score:  0.602666164967869\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
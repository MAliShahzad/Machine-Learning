{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Programming Assignment 3 Multinomial Logistic Regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/potasali/Machine-Learning/blob/master/Programming_Assignment_3_Multinomial_Logistic_Regression/Programming_Assignment_3_Multinomial_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV2lsq4i0V6i",
        "colab_type": "text"
      },
      "source": [
        "# Programming Assignment 3: Multinomial Logistic Regression\n",
        "\n",
        "## Instructions\n",
        "- The aim of this assignment is to give you an initial hands-on regarding real-life\n",
        "machine learning application.\n",
        "- Use separate training and testing data as discussed in class.\n",
        "- You can only use Python programming language and Jupyter Notebook.\n",
        "- You can only use numpy, matplotlib and are not allowed to use NLTK, scikit-learn\n",
        "or any other machine learning toolkit.\n",
        "- Submit your code as one notebook file (.ipynb) on LMS. The name of file\n",
        "should be your roll number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae7gGYK-0V6j",
        "colab_type": "text"
      },
      "source": [
        "## Problem\n",
        "The purpose of this assignment is to get you familiar with multinomial sentiment\n",
        "classification. By the end of this assignment you will have your very own “Sentiment\n",
        "Analyzer”. You are given with Twitter US Airline Sentiment Dataset that contains around\n",
        "14,640 tweets about airlines labelled as positive, negative and neutral. Your task is to train\n",
        "a Multinomial Logistic Regression classifier on this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFU0__Mo0V6l",
        "colab_type": "text"
      },
      "source": [
        "## 1. Dataset Splitting\n",
        "Instead of a usual random split, we will split the dataset in a stratified fashion. Stratified\n",
        "splitting ensure that the train and test sets have approximately the same percentage of\n",
        "samples of each target class as the complete set. For example, in an 80-20 stratified split\n",
        "80% samples of each class will be in train set and 20% in test set.\n",
        "Implement stratified split and do the 80-20 train-test split of the provided dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3USY6PG0V6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import math \n",
        "import pandas as pd\n",
        "import string\n",
        "import heapq\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsEZpIfT0V67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the dataset\n",
        "df = pd.read_csv (r'Tweets.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2SFlKW70V7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dividing the dataset into train and test data and then furthe dividing it into X and Y. \n",
        "def dataSplit(df):\n",
        "    \n",
        "    # Exctracting data for each class\n",
        "    neu = df[df['airline_sentiment'] == 'neutral']\n",
        "    pos = df[df['airline_sentiment'] == 'positive']\n",
        "    neg = df[df['airline_sentiment'] == 'negative']\n",
        "\n",
        "    # Splitting the dataset into train and test data using 80:20 ratio\n",
        "\n",
        "    # Using the first 80% data entries as train data\n",
        "    neu_train = neu[0:int( len(neu)*0.8 )]\n",
        "    pos_train = pos[0:int( len(pos)*0.8 )]\n",
        "    neg_train = neg[0:int( len(neg)*0.8 )]\n",
        "\n",
        "    # Using the last 20% data entries as test data\n",
        "    neu_test = neu[int(len(neu)*0.8): len(neu)]\n",
        "    pos_test = pos[int(len(pos)*0.8): len(pos)]\n",
        "    neg_test = neg[int(len(neg)*0.8): len(neg)]\n",
        "\n",
        "    # Combining the three classes to get train and test data further divided by X and Y\n",
        "    X_train = np.concatenate((neu_train['text'], pos_train['text'], neg_train['text']))\n",
        "    Y_train = np.concatenate((neu_train['airline_sentiment'], pos_train['airline_sentiment'], neg_train['airline_sentiment']))\n",
        "    X_test = np.concatenate((neu_test['text'], pos_test['text'], neg_test['text']))\n",
        "    Y_test = np.concatenate((neu_test['airline_sentiment'], pos_test['airline_sentiment'], neg_test['airline_sentiment']))\n",
        "    \n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = dataSplit(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myTbZ_DB0V7I",
        "colab_type": "text"
      },
      "source": [
        "## 2. Dataset Preprocessing\n",
        "We’ll represent a tweet as a bag-of-words, that is, an unordered set of words with their\n",
        "position ignored, keeping only their frequency in the tweet. <br>\n",
        "Please note that in our case the vocabulary might be in thousands, so we will use text cleaning techniques such as ignore case, punctuation and frequent (stop) words like “a”, ”an”, “the” etc. to reduce the size of vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXun11vc0V7I",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 One Hot Encoding & Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p1rN_eP0V7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One hot encoding for each label in the training dataset\n",
        "# Using \n",
        "#   first column as neutral\n",
        "#   second column as positive\n",
        "#   third column as negative\n",
        "def oheEncoding(data, arr):\n",
        "    '''\n",
        "    Returns a matrix where each sample in y is represented\n",
        "           as a row, and each column represents the class label in\n",
        "           the one-hot encoding scheme.\n",
        "    '''\n",
        "    for label in data:\n",
        "\n",
        "        if label == 'neutral':\n",
        "            arr.append([1,0,0])\n",
        "\n",
        "        elif label == 'positive':\n",
        "            arr.append([0,1,0])\n",
        "\n",
        "        elif label == 'negative':\n",
        "            arr.append([0,0,1])\n",
        "\n",
        "    arr = np.asarray(arr)\n",
        "\n",
        "# label encoding for each label in the test dataset\n",
        "def labelEncoding(data, arr):\n",
        "    for label in data:\n",
        "\n",
        "        # Labeling neutral as 0\n",
        "        if label == 'neutral':\n",
        "            arr.append(0)\n",
        "\n",
        "        # Labeling positive as 1\n",
        "        elif label == 'positive':\n",
        "            arr.append(1)\n",
        "\n",
        "        # Labeling negative as 2\n",
        "        elif label == 'negative':\n",
        "            arr.append(2)\n",
        "\n",
        "    arr = np.asarray(arr)\n",
        "\n",
        "\n",
        "def yProcess(Y_train, Y_test):\n",
        "    ytr = []\n",
        "    yte = []\n",
        "    oheEncoding(Y_train, ytr)\n",
        "    labelEncoding(Y_test, yte)\n",
        "    return ytr, yte\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7qCEl6G0V7S",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Data Cleaning & Bag of Words Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvUb02y20V7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Cleaning the dataset\n",
        "def dataCleaning(data):\n",
        "    arr = []\n",
        "\n",
        "    # Iterating through the entire X data\n",
        "    for text in data:\n",
        "\n",
        "        # Converting text into lower case\n",
        "        text = text.lower()\n",
        "\n",
        "        # Replaces all non word characters (this excludes characters from a to Z and digits from 0-9) with a space\n",
        "        text = re.sub(r'\\W',' ',text)\n",
        "\n",
        "        # Replaces multiple spaces with a single space\n",
        "        text = re.sub(r'\\s+',' ',text)\n",
        "\n",
        "        # Appends the preprocessesed data to an array\n",
        "        arr.append(text)\n",
        "\n",
        "    arr = pd.DataFrame(arr, columns=[\"text\"])\n",
        "    return arr\n",
        "\n",
        "# Extracting the entire vocabulary from the dataset\n",
        "def dictionary(tweets):\n",
        "\n",
        "    # Initializing the word dictionary\n",
        "    wordfreq = {}\n",
        "\n",
        "    # Iterating through the entire X data\n",
        "    for tweet in tweets:\n",
        "\n",
        "        # Tokenizing each word from the data\n",
        "        tokens = tweet.split()\n",
        "\n",
        "        # Iterating through every word\n",
        "        for token in tokens:\n",
        "\n",
        "            # Update the count of the word in the dictionary while dynamically adding new words to the dictionary\n",
        "            if token not in wordfreq.keys():\n",
        "                wordfreq[token] = 1\n",
        "            else:\n",
        "                wordfreq[token] += 1\n",
        "\n",
        "    return wordfreq\n",
        "\n",
        "# Creating a bag of words array\n",
        "def bow(tweets, vocab):\n",
        "\n",
        "    #initializing the bag of word vector\n",
        "    tweet_vectors = []\n",
        "\n",
        "    # Iterating through the entire X data\n",
        "    for tweet in tweets:\n",
        "\n",
        "        # Tokenizing each word from the data\n",
        "        tweet_tokens = tweet.split()\n",
        "\n",
        "        sent_vec = []\n",
        "\n",
        "        # Iterating through every word from the entire vocabulary\n",
        "        for token in vocab:\n",
        "\n",
        "            # Update the count of the each word in the tweet for the given vocabulary\n",
        "            if token in tweet_tokens:\n",
        "                sent_vec.append(tweet.count(token))\n",
        "            else:\n",
        "                sent_vec.append(0)\n",
        "        \n",
        "        # Appending the counts to the bag of word vector\n",
        "        tweet_vectors.append(sent_vec)\n",
        "\n",
        "    return tweet_vectors\n",
        "\n",
        "def xProcess(X_train, X_test):\n",
        "\n",
        "    # Passing the datasets through the dataCleaing function \n",
        "    xtr = dataCleaning(X_train)\n",
        "    xte = dataCleaning(X_test)\n",
        "    \n",
        "    # Finding the vocabulary for the train data\n",
        "    wordfreq = dictionary(xte.text)\n",
        "\n",
        "    # Sorting the vocabulary using heapsort for the counts\n",
        "    mostfreq = heapq.nlargest(5989, wordfreq, key=wordfreq.get)\n",
        "\n",
        "    # Removing stopwords fromo the vocabulary\n",
        "    mostfreq_stop = [word for word in mostfreq if not word in stopwords.words()]\n",
        "\n",
        "    # Finding the bag of words for the datasets\n",
        "    xtr = bow(xtr.text, mostfreq_stop)\n",
        "    xte = bow(xte.text, mostfreq_stop)\n",
        "    \n",
        "    xtr = pd.DataFrame(xtr)\n",
        "    xte = pd.DataFrame(xte)\n",
        "    \n",
        "    xtr = xtr.to_numpy()\n",
        "    xte = xte.to_numpy()\n",
        "    \n",
        "    return xtr, xte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6KKQ6580V7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataSetPreProcess(X_train, X_test, Y_train, Y_test):\n",
        "    ytr, yte = yProcess(Y_train, Y_test)\n",
        "    xtr, xte = xProcess(X_train, X_test)\n",
        "    return xtr, ytr, xte, yte\n",
        "xtr, ytr, xte, yte = dataSetPreProcess(X_train, X_test, Y_train, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLIReMMi0V7l",
        "colab_type": "code",
        "colab": {},
        "outputId": "b41b43c6-bb4a-4ad0-bc9c-a590cd02107d"
      },
      "source": [
        "xtr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfTwlb600V7t",
        "colab_type": "text"
      },
      "source": [
        " ## 3. Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn20vlmA0V7v",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Softmax function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEhtLs4O0V7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Softmax function is an activation function that turns numbers into probabilities which sum to one.\n",
        "def softmax(z):\n",
        "    smax = (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
        "    return smax\n",
        "\n",
        "# Hypothesis Function h(x) predicts the label of a set of data\n",
        "def h_x(X, W):\n",
        "\n",
        "    # Taking dot product of the Feature Vector 'X' and the weights 'W'\n",
        "    hx = (np.dot(X,W))\n",
        "\n",
        "    return hx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS-gaLfg0V72",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Cross-entropy loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w-S3cMU0V73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cross Entropy loss measures the performance of our classification model.\n",
        "def cross_entropy(hx, y):\n",
        "    ce = - np.sum(np.log(hx) * (y), axis=1)\n",
        "    return J\n",
        "\n",
        "# Takes the mean of the cross entropy loss\n",
        "def cost(hx, y):\n",
        "    J = np.mean(cross_entropy(hx, y))\n",
        "    return J"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KZ-4wmR0V78",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Mini-batch Gradient Descent with batch size of 32 samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUyOZ-ax0V78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def miniBatchGD(X, Y, alpha, iters):\n",
        "    \"\"\"Mini Batch Gradient Descent.\n",
        "\n",
        "        Parameters\n",
        "        ------------\n",
        "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of features.\n",
        "        Y : {array-like, sparse matrix}, shape = [n_samples, 1]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "        alpha : float (default: 0.001)\n",
        "            Learning rate (between 0.0 and 1.0)\n",
        "        epochs : int (default: 100)\n",
        "            Passes over the training dataset.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    # number of samples\n",
        "    m = (xtr.shape)[0]\n",
        "    \n",
        "    # number of features\n",
        "    n = (xtr.shape)[1] \n",
        "\n",
        "    # W : 2d-array, shape={n_features, 1}\n",
        "    # Model weights after fitting.\n",
        "    # Setting random state for shuffling and initializing the weights.\n",
        "    W = np.full((n, 3), 0.1)\n",
        "    \n",
        "    # Iterating for a predefined iter\n",
        "    for epoch in range (iters):\n",
        "\n",
        "        # Iterating through batches of 32\n",
        "        for i in range(365):\n",
        "\n",
        "            # Extracting the next batch of 32 samples from the dataset\n",
        "            b = i * 32\n",
        "            batchX = X[b : b + 32]\n",
        "            batchY = Y[b : b + 32]\n",
        "            \n",
        "            # Predicting the labels using our model\n",
        "            ni = h_x(batchX, W)\n",
        "\n",
        "            # Applying softmax on the predicted value\n",
        "            smax = softmax(ni) \n",
        "\n",
        "            # Updating the weights using gradient descent with the predefined alpha\n",
        "            diff = smax - batchY\n",
        "            mse = np.mean(diff, axis=0)\n",
        "            grad = np.dot(batchX.T, diff)\n",
        "            W -= (alpha * grad)\n",
        "\n",
        "    return W\n",
        "\n",
        "W = miniBatchGD(xtr,ytr,0.001,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0B_zNZl0V8D",
        "colab_type": "text"
      },
      "source": [
        "### 3.4 Prediction function \n",
        "to predict whether the tweet is positive, negative or neutral using learned multinomial logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oct6lVtk0V8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns the class with the maximum probability\n",
        "def to_classlabel(z):\n",
        "    label = z.argmax(axis=1)\n",
        "    return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07hElfe20V8K",
        "colab_type": "text"
      },
      "source": [
        "## 4. Evaluation report\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8oHPaqk0V8L",
        "colab_type": "code",
        "colab": {},
        "outputId": "f35b219b-090e-4bed-88ae-9a6cf9452148"
      },
      "source": [
        "def eval(prediction, expected):\n",
        "    \n",
        "    # Initializing the confusion matrix along with all the outcomes\n",
        "    tp = [0,0,0]\n",
        "    tn = [0,0,0]\n",
        "    fp = [0,0,0]\n",
        "    fn = [0,0,0]\n",
        "    CM = np.full((3,3), 0)\n",
        "\n",
        "    # Updating the confusion matrix\n",
        "    for E, P in zip(expected, prediction): \n",
        "        CM[P][E] += 1\n",
        "    \n",
        "    # Updating values for the true-positives, true-negatives, false-positives, false-negatives\n",
        "    for i in range(3):\n",
        "        tp[i] = CM[i][i]\n",
        "        fp[i] = (CM[:, i]).sum() - tp[i]\n",
        "        fn[i] = (CM[i, :]).sum() - tp[i]\n",
        "        tn[i] = (CM.sum()) - tp[i] - fp[i] - fn[i]\n",
        "\n",
        "    # Displaying the Confusion Matrix\n",
        "    CM = pd.DataFrame(data=CM)\n",
        "    CM.columns = ['neutral', 'positive', 'negative']\n",
        "    CM.rename(index={0:'neutral',1:'positive',2:'negative'}, inplace=True)\n",
        "    print(\"Confusion Matrix\\n\\n\", CM)\n",
        "\n",
        "    # Finding the pooled confusion matrix\n",
        "    poolCM = np.array([[sum(tp), sum(fp)],[sum(fn), sum(tn)]])\n",
        "    print(\"\\nPooled: \\n\", poolCM)\n",
        "    \n",
        "    # Micro-Averaging Precision \n",
        "    print(\"\\nMicro-Average Precision: \", (sum(tp)/(sum(fp)+sum(tp))))\n",
        "    \n",
        "    # Micro-Averaging Recall\n",
        "    print(\"Micro-Average Recall: \", (sum(tp)/(sum(fn)+sum(tp))))\n",
        "    \n",
        "    # Micro-Averaging Accuracy\n",
        "    print(\"Micro-Average Accuracy: \", ((sum(tp)+sum(tn))/(sum(fn)+sum(tp)+sum(fp)+sum(tn))))\n",
        "    \n",
        "    # Micro-Averaging F1 score\n",
        "    miP = (sum(tp)/(sum(fp)+sum(tp)))\n",
        "    miR = (sum(tp)/(sum(fn)+sum(tp)))\n",
        "    f1_score = (2 * miP * miR) / (miP + miR)\n",
        "    print(\"Micro-Average F1-Score: \", f1_score)\n",
        "    \n",
        "    # Finding Precison, Accuracy and Recall for each class\n",
        "    maP = []\n",
        "    maR = []\n",
        "    maA = []\n",
        "    for i in range(3):\n",
        "        maP.append(tp[i] / (tp[i] + fp[i]))\n",
        "        maR.append(tp[i] / (tp[i] + fn[i]))\n",
        "        maA.append((tp[i] + tn[i]) / (tp[i] + tn[i] + fp[i] + fn[i]))\n",
        "\n",
        "    # Macro-Average Precision\n",
        "    _maP = (sum(maP))/3\n",
        "    print(\"\\nMacro-Average Precision: \", _maP)\n",
        "\n",
        "    # Macro-Average Recall\n",
        "    _maR = (sum(maR))/3\n",
        "    print(\"Macro-Average Recall: \", _maR)\n",
        "\n",
        "    # Macro-Average Accuracy\n",
        "    _maA = (sum(maA))/3\n",
        "    print(\"Macro-Average Accuracy: \", _maA)\n",
        "\n",
        "    # Macro-Average F1 score\n",
        "    f1_score = (2 * _maP * _maR) / (_maP + _maR)\n",
        "    print(\"Macro-Average F1-Score: \", f1_score)\n",
        "\n",
        "# Using our model to predict labels from the test data\n",
        "ni = h_x(xte,W)\n",
        "smax = softmax(ni)\n",
        "predicted = to_classlabel(smax)\n",
        "eval(predicted, yte)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "\n",
            "           neutral  positive  negative\n",
            "neutral       214        27        43\n",
            "positive       73       275        42\n",
            "negative      333       171      1751\n",
            "\n",
            "Pooled: \n",
            " [[2240  689]\n",
            " [ 689 5169]]\n",
            "\n",
            "Micro-Average Precision:  0.7647661317855924\n",
            "Micro-Average Recall:  0.7647661317855924\n",
            "Micro-Average Accuracy:  0.8431774211903948\n",
            "Micro-Average F1-Score:  0.7647661317855925\n",
            "\n",
            "Macro-Average Precision:  0.6267534476211646\n",
            "Macro-Average Recall:  0.7450486686488061\n",
            "Macro-Average Accuracy:  0.7450486686488061\n",
            "Macro-Average F1-Score:  0.6808005559736282\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}